{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23fb3a1a",
   "metadata": {},
   "source": [
    "# Generated DDL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a4544b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2260547533.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\labot\\AppData\\Local\\Temp\\ipykernel_11116\\2260547533.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    CREATE TABLE `Customer` (\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "CREATE TABLE `Customer` (\n",
    "  `customer_id` integer,\n",
    "  `first_name` varchar (100),\n",
    "  `last_name` varchar (100),\n",
    "  `street_address` varchar (100),\n",
    "  `phone_number` varchar (100),\n",
    "  `state` varchar (100),\n",
    "  `email` varchar (100),\n",
    "  `zip` varchar (100),\n",
    "  `city ` varchar (100)\n",
    ");\n",
    "\n",
    "CREATE TABLE `Employee` (\n",
    "  `emploee_id` integer,\n",
    "  `store_id` integer,\n",
    "  `first_name` varchar (100),\n",
    "  `last_name` varchar (100)\n",
    ");\n",
    "\n",
    "CREATE TABLE `Inventory` (\n",
    "  `product_id` integer,\n",
    "  `product_name` varchar (100),\n",
    "  `quantity` integer,\n",
    "  `items_sold` integer\n",
    ");\n",
    "\n",
    "CREATE TABLE `Store (Homers Choice)` (\n",
    "  `store_id` integer,\n",
    "  `street_address` varchar (100),\n",
    "  `city` varchar (100),\n",
    "  `state` varchar (100),\n",
    "  `zip` varchar (100)\n",
    ");\n",
    "\n",
    "CREATE TABLE `Transaction` (\n",
    "  `transaction_id` integer,\n",
    "  `date` timestamp,\n",
    "  `amount` varchar (100)\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4e913",
   "metadata": {},
   "source": [
    "# Found customer data on Kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f22193",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kaggle.com/datasets/shwetabh123/mall-customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e613b349",
   "metadata": {},
   "source": [
    "# Generated fake store data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "stores = []\n",
    "\n",
    "\n",
    "def fake_data_generation(records):\n",
    "    fake = Faker('en_US')\n",
    "\n",
    "    for i in range(records):\n",
    "        store_id = random.randint(1, 10000)\n",
    "\n",
    "        # Generate data\n",
    "        store_address = fake.address()\n",
    "        city = fake.city()\n",
    "        state = fake.state_abbr()\n",
    "        zip_code = fake.postcode()\n",
    "\n",
    "        stores.append({\"store_id\": store_id, \"address\": store_address, \"city\": city, \"state\": state, \"zip\": zip_code})\n",
    "\n",
    "    stores_df = pd.DataFrame(stores)\n",
    "    store_table = stores_df.to_csv(r\"C:\\Users\\labot\\Downloads\\stores.csv\", index=False)\n",
    "\n",
    "\n",
    "fake_data_generation(500)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02fb016",
   "metadata": {},
   "source": [
    "# Genererated fake employee data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2a3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "\n",
    "# function to create randomized employee data\n",
    "def fake_data_generation(records):\n",
    "    fake= Faker('en_US')\n",
    "    employee = []\n",
    "\n",
    "\n",
    "    employee_id_list = [random.randint(1, 1000) for _ in range(records)]\n",
    "    store_id_list = [random.randint(1, 100) for _ in range(records)]\n",
    "    first_name_list = [fake.first_name() for _ in range(records)]\n",
    "    last_name_list = [fake.last_name() for _ in range(records)]\n",
    "    employee.append({'employee_id': employee_id_list,\n",
    "                   'store_id': store_id_list,\n",
    "                   'first_name': first_name_list,\n",
    "                   'last_name': last_name_list })\n",
    "\n",
    "    employee_df = pd.DataFrame(fake_data_generation(500))\n",
    "    employee_table = employee_df.to_csv(r\"C:\\Users\\labot\\Downloads\\employee.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9bbc9b",
   "metadata": {},
   "source": [
    "# Generated fake inventory data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d92d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "def fake_data_generation(records):\n",
    "    fake = Faker('en_US')\n",
    "\n",
    "    products = []\n",
    "\n",
    "    for i in range(records):\n",
    "        product_name = fake.word()\n",
    "        quantity = fake.random_int(min=1, max=100)\n",
    "\n",
    "        products.append({\n",
    "            \"product_id\": fake.uuid4(),\n",
    "            \"product_name\": product_name,\n",
    "            \"quantity\": quantity,\n",
    "            \"items_sold\": fake.random_int(min=0, max=quantity)\n",
    "        })\n",
    "\n",
    "    return products\n",
    "\n",
    "products_df = pd.DataFrame(fake_data_generation(500))\n",
    "products_table = products_df.to_csv(r\"C:\\Users\\labot\\Downloads\\inventory.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc5646b",
   "metadata": {},
   "source": [
    "# Generated fake transaction data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "def fake_transaction_generation(records):\n",
    "    fake = Faker('en_US')\n",
    "\n",
    "    transactions = []\n",
    "\n",
    "    for i in range(records):\n",
    "        transactions.append({\n",
    "            \"transaction_id\": fake.random_int(min=10000, max=99999),\n",
    "            \"date\": fake.date(),\n",
    "            \"amount\":fake.random_int(min=1, max=10000)\n",
    "        })\n",
    "\n",
    "    return transactions\n",
    "\n",
    "transaction_df = pd.DataFrame(fake_transaction_generation(500))\n",
    "transaction_table = transaction_df.to_csv(r\"C:\\Users\\labot\\Downloads\\transactions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe97b1",
   "metadata": {},
   "source": [
    "# Used AWS Glue Studio to correct and conver CSV files to Parquete\n",
    "\n",
    "\n",
    "# Customers file converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeeaba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "# Script generated for node Data Catalog table\n",
    "DataCatalogtable_node1 = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=\"labotskiy-capstone\",\n",
    "    table_name=\"customers\",\n",
    "    transformation_ctx=\"DataCatalogtable_node1\",\n",
    ")\n",
    "\n",
    "# Script generated for node ApplyMapping\n",
    "ApplyMapping_node2 = ApplyMapping.apply(\n",
    "    frame=DataCatalogtable_node1,\n",
    "    mappings=[\n",
    "        (\"customerid\", \"long\", \"customerid\", \"long\"),\n",
    "        (\"genre\", \"string\", \"genre\", \"string\"),\n",
    "        (\"age\", \"long\", \"age\", \"long\"),\n",
    "        (\"`annual income (k$)`\", \"long\", \"`annual income (k$)`\", \"long\"),\n",
    "        (\"`spending score (1-100)`\", \"long\", \"`spending score (1-100)`\", \"long\"),\n",
    "    ],\n",
    "    transformation_ctx=\"ApplyMapping_node2\",\n",
    ")\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node3 = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=ApplyMapping_node2,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"glueparquet\",\n",
    "    connection_options={\n",
    "        \"path\": \"s3://labotskiy-capstone/capstone-parquet/\",\n",
    "        \"partitionKeys\": [],\n",
    "    },\n",
    "    format_options={\"compression\": \"uncompressed\"},\n",
    "    transformation_ctx=\"S3bucket_node3\",\n",
    ")\n",
    "\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f7533",
   "metadata": {},
   "source": [
    "# Employee file converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "# Script generated for node Data Catalog table\n",
    "DataCatalogtable_node1 = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=\"labotskiy-capstone\",\n",
    "    table_name=\"employee\",\n",
    "    transformation_ctx=\"DataCatalogtable_node1\",\n",
    ")\n",
    "\n",
    "# Script generated for node ApplyMapping\n",
    "ApplyMapping_node2 = ApplyMapping.apply(\n",
    "    frame=DataCatalogtable_node1,\n",
    "    mappings=[\n",
    "        (\"employee_id\", \"string\", \"employee_id\", \"string\"),\n",
    "        (\"store_id\", \"long\", \"store_id\", \"long\"),\n",
    "        (\"first_name\", \"string\", \"first_name\", \"string\"),\n",
    "        (\"last_name\", \"string\", \"last_name\", \"string\"),\n",
    "    ],\n",
    "    transformation_ctx=\"ApplyMapping_node2\",\n",
    ")\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node3 = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=ApplyMapping_node2,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"glueparquet\",\n",
    "    connection_options={\n",
    "        \"path\": \"s3://labotskiy-capstone/capstone-parquet/\",\n",
    "        \"partitionKeys\": [],\n",
    "    },\n",
    "    format_options={\"compression\": \"uncompressed\"},\n",
    "    transformation_ctx=\"S3bucket_node3\",\n",
    ")\n",
    "\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa547c8",
   "metadata": {},
   "source": [
    "# Store file converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a7807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "# Script generated for node Data Catalog table\n",
    "DataCatalogtable_node1 = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=\"labotskiy-capstone\",\n",
    "    table_name=\"store\",\n",
    "    transformation_ctx=\"DataCatalogtable_node1\",\n",
    ")\n",
    "\n",
    "# Script generated for node ApplyMapping\n",
    "ApplyMapping_node2 = ApplyMapping.apply(\n",
    "    frame=DataCatalogtable_node1,\n",
    "    mappings=[\n",
    "        (\"store_id\", \"long\", \"store_id\", \"long\"),\n",
    "        (\"address\", \"string\", \"address\", \"string\"),\n",
    "        (\"city\", \"string\", \"city\", \"string\"),\n",
    "        (\"state\", \"string\", \"state\", \"string\"),\n",
    "        (\"zip\", \"long\", \"zip\", \"long\"),\n",
    "    ],\n",
    "    transformation_ctx=\"ApplyMapping_node2\",\n",
    ")\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node3 = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=ApplyMapping_node2,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"glueparquet\",\n",
    "    connection_options={\n",
    "        \"path\": \"s3://labotskiy-capstone/capstone-parquet/\",\n",
    "        \"partitionKeys\": [],\n",
    "    },\n",
    "    transformation_ctx=\"S3bucket_node3\",\n",
    ")\n",
    "\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc816feb",
   "metadata": {},
   "source": [
    "# Inventory file converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0021a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "# Script generated for node Data Catalog table\n",
    "DataCatalogtable_node1 = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=\"labotskiy-capstone\",\n",
    "    table_name=\"inventory\",\n",
    "    transformation_ctx=\"DataCatalogtable_node1\",\n",
    ")\n",
    "\n",
    "# Script generated for node ApplyMapping\n",
    "ApplyMapping_node2 = ApplyMapping.apply(\n",
    "    frame=DataCatalogtable_node1,\n",
    "    mappings=[\n",
    "        (\"product_id\", \"string\", \"product_id\", \"string\"),\n",
    "        (\"product_name\", \"string\", \"product_name\", \"string\"),\n",
    "        (\"quantity\", \"long\", \"quantity\", \"long\"),\n",
    "        (\"items_sold\", \"long\", \"items_sold\", \"long\"),\n",
    "    ],\n",
    "    transformation_ctx=\"ApplyMapping_node2\",\n",
    ")\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node3 = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=ApplyMapping_node2,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"glueparquet\",\n",
    "    connection_options={\n",
    "        \"path\": \"s3://labotskiy-capstone/capstone-parquet/\",\n",
    "        \"partitionKeys\": [],\n",
    "    },\n",
    "    format_options={\"compression\": \"uncompressed\"},\n",
    "    transformation_ctx=\"S3bucket_node3\",\n",
    ")\n",
    "\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df23931",
   "metadata": {},
   "source": [
    "# Transactions file converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e06d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "# Script generated for node Data Catalog table\n",
    "DataCatalogtable_node1 = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=\"labotskiy-capstone\",\n",
    "    table_name=\"transactions\",\n",
    "    transformation_ctx=\"DataCatalogtable_node1\",\n",
    ")\n",
    "\n",
    "# Script generated for node ApplyMapping\n",
    "ApplyMapping_node2 = ApplyMapping.apply(\n",
    "    frame=DataCatalogtable_node1,\n",
    "    mappings=[\n",
    "        (\"transaction_id\", \"long\", \"transaction_id\", \"varchar\"),\n",
    "        (\"date\", \"string\", \"date\", \"string\"),\n",
    "        (\"amount\", \"long\", \"amount\", \"varchar\"),\n",
    "    ],\n",
    "    transformation_ctx=\"ApplyMapping_node2\",\n",
    ")\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node3 = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=ApplyMapping_node2,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"glueparquet\",\n",
    "    connection_options={\"path\": \"s3://labotskiy-capstone\", \"partitionKeys\": []},\n",
    "    format_options={\"compression\": \"uncompressed\"},\n",
    "    transformation_ctx=\"S3bucket_node3\",\n",
    ")\n",
    "\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95007704",
   "metadata": {},
   "source": [
    "# Redshift warehouse DDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b0dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "create external schema capstone\n",
    "from data catalog\n",
    "database 'dev'\n",
    "iam_role 'arn:aws:iam::701187063124:role/Redshift-Capstone'\n",
    "create external database if not exists;\n",
    "\n",
    "CREATE EXTERNAL TABLE capstone.customers (\n",
    "  customerid varchar(100),\n",
    "  gender varchar(100),\n",
    "  age varchar(100),\n",
    "  annual_income varchar(100),\n",
    "  spending_score varchar(100)\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://labotskiy-capstone/capstone-parquet/customers/';\n",
    "\n",
    "SELECT * FROM \"dev\".\"capstone\".\"customers\";\n",
    "\n",
    "CREATE EXTERNAL TABLE capstone.employees (\n",
    "  employeeid varchar(100),\n",
    "  storeid varchar(100),\n",
    "  firstname varchar(100),\n",
    "  lastname varchar(100)\n",
    " )\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://labotskiy-capstone/capstone-parquet/employees/';\n",
    "\n",
    "SELECT * FROM \"dev\".\"capstone\".\"employees\";\n",
    "\n",
    "CREATE EXTERNAL TABLE capstone.store (\n",
    "  storeid varchar(100),\n",
    "  address varchar(100),\n",
    "  city varchar(100),\n",
    "  state varchar(100),\n",
    "  zip varchar(100)\n",
    " )\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://labotskiy-capstone/capstone-parquet/store/';\n",
    "\n",
    "SELECT * FROM \"dev\".\"capstone\".\"store\";\n",
    "\n",
    "CREATE EXTERNAL TABLE capstone.inventory (\n",
    "  productid varchar(100),\n",
    "  productname varchar(100),\n",
    "  quantity varchar(100),\n",
    "  itemssold varchar(100)\n",
    " )\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://labotskiy-capstone/capstone-parquet/inventory/';\n",
    "\n",
    "SELECT * FROM \"dev\".\"capstone\".\"inventory\";\n",
    "\n",
    "CREATE EXTERNAL TABLE capstone.transactions (\n",
    "  transactionid varchar(100),\n",
    "  date date,\n",
    "  ammount varchar(100)\n",
    " )\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://labotskiy-capstone/capstone-parquet/transactions/';\n",
    "\n",
    "SELECT * FROM \"dev\".\"capstone\".\"transactions\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea5589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
